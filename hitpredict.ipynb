{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import seaborn as sns\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n", "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n", "from sklearn.impute import SimpleImputer\n", "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, VotingClassifier\n", "from sklearn.svm import SVC\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, auc, \\\n", "    ConfusionMatrixDisplay, precision_recall_curve, average_precision_score\n", "from sklearn.model_selection import learning_curve"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.pipeline import Pipeline\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.feature_selection import SelectFromModel"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["data = pd.read_csv('C:/Users/Water/PycharmProjects/HitPredictorProject/archive/dataset-of-10s.csv')\n", "X = data.drop(['track', 'artist', 'uri', 'target'], axis=1)\n", "y = data['target']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Preprocessing pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["numeric_transformer = Pipeline(steps=[\n", "    ('imputer', SimpleImputer(strategy='median')),\n", "    ('poly', PolynomialFeatures(degree=2)),\n", "    ('scaler', StandardScaler())\n", "])\n", "preprocessor = ColumnTransformer(transformers=[\n", "    ('num', numeric_transformer, X.columns)\n", "])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Model pipeline"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Pipeline(steps=[\n", "    ('preprocessor', preprocessor),\n", "    ('feature_selector', SelectFromModel(GradientBoostingClassifier(), threshold='median')),\n", "    ('classifier', GradientBoostingClassifier())\n", "])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hyperparameter tuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_dist = {\n", "    'classifier__n_estimators': [100, 200],\n", "    'classifier__learning_rate': [0.01, 0.1, 0.2],\n", "    'classifier__max_depth': [3, 5, 7],\n", "    'classifier__subsample': [0.8, 1.0]\n", "}\n", "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=3, scoring='precision', verbose=3, random_state=42,\n", "                                   n_jobs=-1)\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n", "random_search.fit(X_train, y_train)\n", "best_model = random_search.best_estimator_"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluating the best model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictions = best_model.predict(X_test)\n", "predicted_probs = best_model.predict_proba(X_test)[:, 1]\n", "print(\"Best parameters:\", random_search.best_params_)\n", "print(classification_report(y_test, predictions))\n", "print(confusion_matrix(y_test, predictions))\n", "print(\"ROC AUC Score:\", roc_auc_score(y_test, predicted_probs))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot ROC Curve"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fpr, tpr, _ = roc_curve(y_test, predicted_probs)\n", "plt.figure()\n", "plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc_score(y_test, predicted_probs))\n", "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n", "plt.xlim([0.0, 1.0])\n", "plt.ylim([0.0, 1.05])\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('Receiver operating characteristic')\n", "plt.legend(loc=\"lower right\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define additional models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rf = RandomForestClassifier(n_estimators=100, random_state=42)\n", "svc = SVC(probability=True, kernel='linear', random_state=42)\n", "log_reg = LogisticRegression(max_iter=10000, solver='lbfgs', random_state=42)\n", "gb = best_model  # using best_model for Gradient Boosting"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rf.fit(X_train, y_train)  # Fit RandomForest with training data\n", "svc.fit(X_train, y_train)\n", "log_reg.fit(X_train, y_train)\n", "# Voting Classifier setup\n", "voting_clf = VotingClassifier(\n", "    estimators=[('gb', gb), ('rf', rf), ('svc', svc), ('log_reg', log_reg)],\n", "    voting='hard'\n", ")\n", "voting_clf.fit(X_train, y_train)  # Fit the Voting Classifier on the training data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluating all models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_model_probabilities(model, X_test):\n", "    \"\"\"Get model probabilities. Handles decision_function if predict_proba is unavailable.\"\"\"\n", "    try:\n", "        if hasattr(model, 'predict_proba'):\n", "            return model.predict_proba(X_test)[:, 1]\n", "        elif hasattr(model, 'decision_function'):\n", "            df = model.decision_function(X_test)\n", "            return (df - df.min()) / (df.max() - df.min())  # Scale to [0,1]\n", "        else:\n", "            raise ValueError(f\"{model.__class__.__name__} does not support probability or decision function.\")\n", "    except Exception as e:\n", "        print(f\"Error in getting model probabilities: {e}\")\n", "        return None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate and plot ROC AUC for each model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = [\n", "    ('Gradient Boosting', best_model),\n", "    ('Random Forest', rf),\n", "    ('SVM', svc),\n", "    ('Logistic Regression', log_reg),\n", "    ('Voting Classifier', voting_clf)\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for name, model in models:\n", "    probas = get_model_probabilities(model, X_test)\n", "    if probas is not None:\n", "        fpr, tpr, _ = roc_curve(y_test, probas)\n", "        roc_auc = auc(fpr, tpr)\n", "        plt.plot(fpr, tpr, label=f'{name} (area = {roc_auc:.2f})')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot([0, 1], [0, 1], 'k--')\n", "plt.xlabel('False Positive Rate')\n", "plt.ylabel('True Positive Rate')\n", "plt.title('ROC Curve Comparison')\n", "plt.legend(loc='lower right')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot Precision-Recall curve for each model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(10, 8))\n", "for name, model in models:\n", "    probas = get_model_probabilities(model, X_test)\n", "    if probas is not None:\n", "        precision, recall, _ = precision_recall_curve(y_test, probas)\n", "        ap = average_precision_score(y_test, probas)\n", "        plt.plot(recall, precision, label=f'{name} (AP = {ap:.2f})')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel('Recall')\n", "plt.ylabel('Precision')\n", "plt.title('Precision-Recall Curve Comparison')\n", "plt.legend(loc='lower right')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot learning curve for the best model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_sizes, train_scores, test_scores = learning_curve(best_model, X, y, cv=3,\n", "                                                        train_sizes=np.linspace(.1, 1.0, 5), verbose=0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_scores_mean = np.mean(train_scores, axis=1)\n", "train_scores_std = np.std(train_scores, axis=1)\n", "test_scores_mean = np.mean(test_scores, axis=1)\n", "test_scores_std = np.std(test_scores, axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure()\n", "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1,\n", "                 color=\"r\")\n", "plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1,\n", "                 color=\"g\")\n", "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n", "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.xlabel(\"Training examples\")\n", "plt.ylabel(\"Score\")\n", "plt.title(\"Learning Curve for Best Model\")\n", "plt.legend(loc=\"best\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Confusion Matrix for best model using ConfusionMatrixDisplay"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["cm = confusion_matrix(y_test, best_model.predict(X_test))\n", "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n", "disp.plot(cmap='Blues')\n", "plt.title('Confusion Matrix for Best Model')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Heatmap of the confusion matrix for the best model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n", "plt.title('Heatmap of Confusion Matrix for Best Model')\n", "plt.xlabel('Predicted Label')\n", "plt.ylabel('True Label')\n", "plt.show()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}